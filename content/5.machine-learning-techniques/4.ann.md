---
title: "4. Artificial Neural Network"
description: "A very basic 1-hidden-layer ANN."
head.image: "/neko.png"
date: "2022-05-29"
---

# Experiment 4 - Artificial Neural Network

| Experiment | 4                                               |
| ---------- | ----------------------------------------------- |
| Title      | Artificial Neural Network using BackPropagation |

## Algorithm

- Create 3 layers:
  - An input layer.
  - A hidden layer.
  - An output layer.
- Initialize weights and biases for the hidden layer and output layer.
- Implement forward propagation using the Sigmoid function.
- Implement backpropagation to update gradients.
- Repeat for the required number of epochs.

## Program

- ANNs require matrix math, so we import numpy.
  ```py
  import numpy
  ```
- Define our activation function (sigmoid).
  ```py
  def sigmoid(x):
    return 1 / (1 + np.exp(-x))

  # derivative of sigmoid for calculating gradient
  def d_sigmoid(x):
    return x * (1 - x)
  ```
- Now we initialize our inputs and outputs.
  ```py
  X = np.array(([2, 9], [1, 5], [3, 6]), dtype = float)
  y = np.array(([92], [86], [89]), dtype = float)
  ```
- Let's normalize the inputs and outputs.
  ```py
  X = X / np.amax(X, axis = 0)
  y = y / 100
  ```
- Specify parameters for the neural network:
  ```py
  # Train for 7000 iterations
  epochs = 7000
  # Set learning rate to a small value to avoid overfitting
  learning_rate = 0.1
  # our input has two dimensions
  input_size = 2 
  # We are using a hidden layer with 3 neurons
  hidden_size = 3 
  # We need a 1-dimensional output
  output_size = 1
  ```
- Initialize weights and biases to random numbers.
  ```py
  # Weights and bias of hidden layer
  weights_h = np.random.uniform(size = (input_size, hidden_size))
  bias_h = np.random.uniform(size = (1, hidden_size))

  # Weights and bias of output layer
  weights_out = np.random.uniform(size = (hidden_size, output_size))
  bias_out = np.random.uniform(size = (1, output_size))
  ```
- Train the neural network.
  ```py
  for i in range(epochs):
    # Forward propagation
    hidden_pred = sigmoid(np.dot(X, weights_h) + bias_h)
    output_pred = sigmoid(np.dot(hidden_pred, weights_out) + bias_out)

    # Back propagation for gradient descent
    output_err = y - output_pred
    output_gradients = d_sigmoid(output_pred)
    d_output = output_err * output_gradients

    hidden_err = d_output.dot(weights_out.T)
    hidden_gradients = d_sigmoid(hidden_pred)
    d_hidden = hidden_err * hidden_gradients

    # Update weights and bias with back propagation
    weights_out += hidden_pred.T.dot(d_output) * learning_rate
    bias_out += np.sum(d_output, axis=0, keepdims=True) * learning_rate
    weights_h += X.T.dot(d_hidden) * learning_rate
    bias_h += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate
  ```
- Compare the predicted `output_pred` with our actual `output`.
  ```py
  print("Input: \n" + str(X))
  print("Actual Output: \n" + str(y))
  print("Predicted Output: \n" ,output_pred)
  ```
- This will output the following:
  ```ts
  Input: 
    [
      [0.66666667 1.        ]
      [0.33333333 0.55555556]
      [1.         0.66666667]
    ]
  Actual Output: 
    [
      [0.92]
      [0.86]
      [0.89]
    ]
  Predicted Output: 
    [
      [0.89567704]
      [0.87563352]
      [0.89729249]
    ]
  ```
